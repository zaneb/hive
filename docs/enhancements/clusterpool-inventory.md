# Clusterpool for on-prem cloud providers

[HIVE-1576](https://issues.redhat.com/browse/HIVE-1367)

- [ClusterPool Inventory](#clusterpool-inventory)
    - [User Stories](#user-stories)
    - [Problem Statement](#problem-statement)
    - [Proposal](#proposal)
        - [Summary](#summary)
        - [`ClusterPool.Spec.Inventory`](#clusterpoolspecinventory)
        - [How To Use](#how-to-use)
        - [Validation](#validation)
        - [`Size` and `MaxSize`](#size-and-maxsize)
        - [Pool Version](#pool-version)
        - [Handling Inventory Updates](#handling-inventory-updates)
            - [Adding An Inventory](#adding-an-inventory)
            - [Adding An Entry](#adding-an-entry)
            - [Removing An Entry](#removing-an-entry)
            - [Deleting The Inventory](#deleting-the-inventory)
        - [Racing For Names](#racing-for-names)
        - [Fairness](#fairness)
        
## Summary

As a cluster administrator, I want to be able to use ClusterPools to provision clusters on on-prem clouds like vSphere, OpenStack and Baremetals.

## Problem Statement
Provisioning a cluster requires configuration unique to that cluster.
For example, the cluster's name is used to build the hostnames for the API and console.
_Someone_ has to handle the DNS that resolves those hostnames to the IPs of the services created by the installer.
One option is to create your ClusterDeployment with `ManageDNS=True` and we'll create a DNSZone on the fly.

But that doesn't work for some cloud providers such a vSphere, OpenStack and Baremetals.

In such cases, one solution is to configure DNS manually and then create your ClusterDeployment with the right `Name` and `BaseDomain` so that the assembled hostnames match those entries. This is fine if you're creating ClusterDeployments by hand, but breaks down if ClusterDeployment names are being generated with random slugs, as is the case with ClusterPools.

## Proposal
### Summary
Allow ClusterPool to accept an inventory of install config json patches to be used when generating ClusterDeployments for the pool.

### `ClusterPool.Spec.Inventory`
Add a field to `ClusterDeployment.Spec` called `Inventory`.

It has an array of object references to InstallConfigPatch custom resource. This CR will have a base64 encoded json patch for default install config generated by clusterpool controller. The json patch will have cluster specific changes, for vSphere this corresponds to configured DNS hostnames

```yaml
spec:
  inventory:
    installConfigPatches:
      - name: foo-install-config-patch
      - name: bar-install-config-patch
      - name: baz-install-config-patch
```

and InstallConfigPatch CR will look like
```yaml
apiVersion: v1
kind: InstallConfigPatch
metadata:
  name: foo-install-config-patch
  namespace: my-project
spec:
  patch: <base64 encoded json patch>    
status:
  clusterDeploymentRef: 
    name: foo
  conditions:
    - lastProbeTime: "2020-11-05T14:49:26Z"
      lastTransitionTime: "2020-11-05T14:49:26Z"
      message: patch is currently in use by cluster deployment foo of clusterpool foo-pool
      reason: PatchInUse
      status: "false"
      type: PatchAvailable
```

if DNS is configured with the name `foo`, The base 64 encoded json content to patch vSphere install config will be as follows

```json
[
  { "op": "replace", "path": "metadata/name", "value": "foo" }
]
```

When adding a ClusterDeployment, if such an `Inventory` is present, ClusterPool controller will:
- Load up the inventory list.
- For each reference it will do a GET to fetch InstallConfigPatch and check the if the PatchAvailable condition is true.
- For the first InstallConfigPatch that is available, it will use the json patch in the `spec.patch` field to update the default install config generated by clusterpool. It will also update the status with reference to the cluster deployment that is using it and update the `PatchAvailable` condition to false.
- Set the `spec.clusterPoolReference.installConfigPatchRef`field in the ClusterDeployment with a reference to InstallConfigPatch CR.

Absent an `Inventory`, ClusterPool will continue to use the generated default install config as it does today.

When ClusterDeployment is deleted, ClusterPool controller will: 
- Find out if ClusterDeployment used an InstallConfigPatch from `spec.ClusterPoolReference.InstallConfigPatchRef`field.
- If yes, we simply update InstallConfigPatch, setting the `spec.clusterDeploymentRef` to nil and `PatchAvailable` to true.  

### How To Use
For the VSphere case, this allows the administrator to:
- Preconfigure DNS with following entries (assuming cluster name is `foo`)
    ```
    10.0.0.10  api.foo.example.com
    10.0.0.11  apps.foo.example.com
    10.0.0.12  console.foo.example.com
    ```
- Create a InstallConfigPatch CR to patch `spec.metadata.name` field of the default install config generated by clusterpool controller. Please refer the section above of a sample CR. The base64 encoded json content in `spec.patch` field should be as follows
    ```json
    [
      { "op": "replace", "path": "metadata/name", "value": "foo" }
    ]
    ```
- Add the name of InstallConfigPatch CR to`clusterPool.spec.inventory.installConfigPatches` list. For InstallConfigPatch with a name `foo-install-config-patch` the clusterpool should be configured as follows
    ```yaml
    spec:
      inventory:
        installConfigPatches:
          - name: foo-install-config-patch
    ```
  
### Validation
Webhook validation will ensure that for a clusterpool
- if `spec.inventory.installConfigPatchs` is specified, it is not an empty list.
- if `spec.inventory.installConfigPatchs` is specified, `spec.maxSize` is equal to or less than length of list `spec.inventory.installConfigPatchs`.
- if `spec.inventory.installConfigPatchs` is specified, `spec.size` is equal to or less than length of list `spec.inventory.installConfigPatchs`.

### `Size` and `MaxSize`
If `Inventory` is used, `ClusterPool.Spec.Size` and `.MaxSize` are implicitly constrained to the length of the list of nonempty `spec.inventory.installConfigPatchs`.
Setting either/both to a smaller number still works as you would expect.

### Pool Version
To make [adding](#adding-an-inventory) and [deleting](#deleting-the-inventory) `Inventory` work sanely, we will adjust the computation of the pool version used for [stale CD detection/replacement](https://issues.redhat.com/browse/HIVE-1058) as follows:
- When `Inventory` is present, append an arbitrary fixed string before the [final hash operation](https://github.com/openshift/hive/blob/0b9229b91e6f8a3c2bf095efbdf017226e69d026/pkg/controller/clusterpool/clusterpool_controller.go#L479).
  (We don't want to recalculate the hash any time the inventory changes, as this would treat *all* existing unclaimed CDs as stale and replace them.)
- When `Inventory` is absent, compute the pool version as before.
  This ensures the version does not change (triggering replacement of all unclaimed CDs) for existing pools when hive is upgraded to include this feature.

### Handling Inventory Updates
#### Adding An Inventory
Adding an `Inventory` to a ClusterPool which previously didn't have one will cause the controller to [recompute the pool version](#pool-version), rendering all existing unclaimed clusters stale, causing them to be replaced gradually.

#### Adding An Entry to the Inventory
If `MaxSize` is unset or less than/equal to the length of the inventory, and `Size` allows, adding a new entry will cause a new ClusterDeployment to be added to the pool.
If the pool is already at `[Max]Size` there is no immediate effect.

#### Removing An Entry to the Inventory
- If the entry is unused (no pool CD with that name exists), this is a no-op.
- If an _unclaimed_ CD exists with that name, we delete it.
  The controller will replace it, assuming an unused entry is available.
- If a _claimed_ CD exists with that name, no-op.

These are conceived to correlate as closely as possible to what happens when editing a pool's `Size`.

#### Deleting The Inventory
This will change the [pool version](#pool-version), rendering existing unclaimed clusters stale and causing the controller to replace them gradually.
The administrator may wish to speed up this process by manually deleting CDs, or scaling the pool size to zero and back.

### Maintaining the lease of the InstallConfigPatch
If two controller pods trying to build a ClusterDeployment for the ClusterPool end up fetching the same installConfigPatch, they will be trying to claim same configured DNS or a hostname - a chaos scenario. To avoid it, we need to maintain a lease for each InstallConfigPatch

To solve the above problem, we will have `status.clusterDeploymentRef` field and `PatchAvailable` condition on InstallConfigPatch.
- When the `PatchAvailable` condition is false and `status.clusterDeploymentRef` is nil, the InstallConfigPatch hasn't been claimed yet.
- When we need to build a new ClusterDeployment, we
    - Load up the InstallConfigPatch list from the inventory.
    - Do a GET on each entry in the list and check if the `PatchAvailable` condition is true
    - For the first InstallConfigPatch that is available, we will use the json patch in the `spec.patch` field to update the default install config generated by the clusterpool.
    - Update the `spec.clusterPoolReference.installConfigPatchRef`field in the ClusterDeployment with a reference to InstallConfigPatch
    - Post an update to the InstallConfigPatch with the reference to the ClusterDeployment. Also update the `PatchAvailable` condition to false.
      Here's where we bounce if two controllers try to grab the same InstallConfigPatch: the loser will bounce the update with a 409; we'll requeue and start over.
    - Proceed with existing algorithm to create new cluster.
- Add a top-level routine to ensure all unavailable InstallConfigPatches have valid ClusterDeployments associated with them.
  If not, attempt to create them through the same flow.
  This is to guard against leaking entries if a controller crashes between updating the lease on InstallConfigPatch and creating the CD.

### Fairness
We will assume it is **not** important that we rotate through the list of supplied install config patches in any particular order, or with any nod to "fairness".
For example: If there are five patches and the usage pattern happens to only use two at a time, there is no guarantee that we will round-robin through all five.
We may reuse the same two over and over, or pick at random, or something else -- any of which has the potential to "starve" some names.
